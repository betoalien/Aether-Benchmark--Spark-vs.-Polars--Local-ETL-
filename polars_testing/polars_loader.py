import os
import time
import polars as pl
import psycopg2
import io

# --- CREDENTIALS ---
DB_CONFIG = {
    "host": "localhost",
    "database": "postgres",
    "user": "postgres",
    "password": "alien666"
}

DB_TABLE = "sales"
# Path to the parquet generated by Polars in the previous step
PARQUET_SOURCE = "POLARS_WAREHOUSE/data.parquet"

def clean_table():
    """Cleans the table before the benchmark to ensure fairness"""
    print(f"üßπ CLEANING TABLE '{DB_TABLE}'...")
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        cur.execute(f"TRUNCATE TABLE {DB_TABLE};")
        conn.commit()
        cur.close()
        conn.close()
        print("‚ú® Table truncated successfully.")
    except Exception as e:
        print(f"‚ö†Ô∏è Error cleaning table: {e}")

def polars_load():
    # 1. Cleanup
    clean_table()

    print(f"üî• STARTING MASSIVE LOAD: PARQUET TO POSTGRESQL...")
    
    # 2. Connection
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()
        print("‚úÖ Connection established with SQL Engine.")
    except Exception as e:
        print(f"‚ùå Error connecting to DB: {e}")
        return

    if not os.path.exists(PARQUET_SOURCE):
        print(f"‚ùå File not found at {PARQUET_SOURCE}. Did you run the CSV -> Parquet conversion?")
        return

    start_total = time.time()
    
    print(f"üéØ Source File: {PARQUET_SOURCE}")

    try:
        # Unlike the previous code that iterated CSVs, here we load the Parquet at once.
        # Polars is incredibly memory efficient for this.
        print("üêª Reading Parquet into memory...", end=" ", flush=True)
        df = pl.read_parquet(PARQUET_SOURCE)
        print(f"‚úÖ Ready ({len(df):,} rows loaded in RAM)")

        # 3. Ingest via COPY STREAM
        print(f"üíâ Injecting data into Postgres...", end=" ", flush=True)
        start_copy = time.time()

        # MAGIC TRICK: In-Memory Buffer
        # We convert the DataFrame to a binary CSV buffer in RAM.
        # This avoids writing a temporary CSV to disk (expensive I/O).
        csv_buffer = io.BytesIO()
        
        # Write without header because COPY FROM STDIN assumes pure data
        df.write_csv(csv_buffer, include_header=False)
        
        # Rewind the buffer to the beginning to read it
        csv_buffer.seek(0)

        # We use copy_expert just like in your original code
        # This pushes raw bytes directly to the DB socket
        cur.copy_expert(
            sql=f"COPY {DB_TABLE} FROM STDIN WITH CSV DELIMITER ','",
            file=csv_buffer
        )
        
        conn.commit() # Commit the massive transaction
        
        duration = time.time() - start_copy
        print(f"‚úÖ ({duration:.2f}s)")

        # 4. Close
        cur.close()
        conn.close()
        
        end_total = time.time()
        print("-" * 60)
        print(f"üöÄ MASSIVE LOAD COMPLETED.")
        print(f"‚è±Ô∏è  Total Time: {end_total - start_total:.2f} seconds")
        print(f"üì¶ Rows Inserted: {len(df):,}")
        print(f"üìä Table: public.{DB_TABLE}")
        print("-" * 60)

    except Exception as e:
        print(f"\n‚ùå CRITICAL FAILURE: {e}")
        if 'conn' in locals() and conn:
            conn.rollback()

if __name__ == "__main__":
    polars_load()